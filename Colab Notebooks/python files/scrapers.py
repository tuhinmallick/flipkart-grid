# -*- coding: utf-8 -*-
"""scrapers.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1m2vXc3QEvY1rXXsv0tDa-2wEbI4H1gIp
"""



from google.colab import drive
drive.mount('/gdrive')

# Commented out IPython magic to ensure Python compatibility.
# %ls

# Commented out IPython magic to ensure Python compatibility.
# %cd '/gdrive/My Drive'

with open('test.txt', 'w') as f:
  f.write('Hello Google Drive!')
!cat '/gdrive/My Drive/foo.txt'

!pip install selenium

from selenium import webdriver
import time
import urllib.request
import os
from selenium.webdriver.common.keys import Keys

!pip install webdriver_manager

from webdriver_manager.chrome import ChromeDriverManager

!apt install chromium-chromedriver

!apt install google-chrome-stable

browser = webdriver.Chrome(ChromeDriverManager().install())

from selenium import webdriver
chrome_options = webdriver.ChromeOptions()
chrome_options.add_argument('--headless')
chrome_options.add_argument('--no-sandbox')
chrome_options.add_argument('--disable-dev-shm-usage')

wd = webdriver.Chrome('/usr/lib/chromium-browser/chromedriver',options=chrome_options)

try:
    os.mkdir("downloads")
except:
    print("Error creating downloads")

wd.get('https://www.google.com/search?q=blue+dress&authuser=0&sxsrf=ALeKk01QekdHQIfFGkxvh4O8qPJ7oXAFsw:1595389554883&source=lnms&tbm=isch&sa=X&ved=2ahUKEwiNzcmd-d_qAhVKyzgGHVyHBAYQ_AUoAXoECAwQAw&biw=1364&bih=578')
"""
white tshirt, green tshirt, black shirt
white dress, black dress, yellow dress, green dress,
"""
value = 0
for num_scrolls in range(30):
  wd.execute_script("scrollBy(" + str(value) + ", +1000);")
  value += 1000
  time.sleep(5)

time.sleep(300)

elem1 = wd.find_elements_by_class_name('islir')
print(elem1)
#for elem in  elem1:
num = 0
for elem in elem1:
    try:
        sub = elem.find_elements_by_tag_name('img')
    except:
        print(elem.text)
        print("No element found")
        continue

    for j in sub:
        src = j.get_attribute('src')
        try:
            if src != None:
                src = str(src)
                print(src)
                num = num + 1
                urllib.request.urlretrieve(src, os.path.join('data/blue_dress', 'image' + str(num) + '.jpg'))
            else:
                raise TypeError
        except:
            print('failed for current url')

#NordStorm

!pip install selenium

from selenium import webdriver
from bs4 import BeautifulSoup
import urllib.parse as urlparse
import os
import json
import requests
import time
from urllib.parse import parse_qs

chrome_options = webdriver.ChromeOptions()
chrome_options.add_argument('--headless')
chrome_options.add_argument('--no-sandbox')
chrome_options.add_argument('--disable-dev-shm-usage')
driver = webdriver.Chrome('/usr/lib/chromium-browser/chromedriver',options=chrome_options)

driver = webdriver.Chrome('/usr/lib/chromium-browser/chromedriver')

# Commented out IPython magic to ensure Python compatibility.
# %cd nordstorm/

categories = ["New Arrivals"]

results = {}
for category in categories:
    url = "https://www.nordstrom.com/browse/men/trend/clothing?breadcrumb=Home%2FMen%2FTopman%20%26%20Men%27s%20Trend%2FClothing"

 # https://www.nordstrom.com/browse/men/new?breadcrumb=Home%2FMen%2FNew%20Arrivals&origin=topnav
    #url = "https://www.nordstrom.com/browse/men/trend/clothing?breadcrumb=Home%2FMen%2F" + category

    #url = "file:///data/Projects/flipkart/flipkart-gird/scrapers/flipkart/test.html"
    results[category] = []
    print(url)
    driver.get("https://www.nordstrom.com/browse/men/trend/clothing?breadcrumb=Home%2FMen%2FTopman%20%26%20Men%27s%20Trend%2FClothing")
    time.sleep(10)
    items = driver.find_elements_by_class_name('QIjwE')
    print(items)
    for i in range(5):
    #for item in items:
        #try:
        item_elem = items[i].find_elements_by_class_name('_1av3_')
        item_url = item_elem[0].get_attribute('href')
        results[category].append(item_url)
        #except:
        #    print("error scraping this item")
    """
        #columns = row.find_elements_by_class_name('_3dqZjq')
        columns = rows[i].find_elements_by_class_name('_3dqZjq')
        for column in columns:
                results[category].append(column.get_attribute('href'))
    """

with open("nordstrom_links.json", 'w') as file:
    json.dump(results, file)

driver.close()

categories = ["trending tshirts", "trending jeans", "trending dress", "trending suits"]

results = {}
for category in categories:
    print(category)
    results[category] = []
    for pageno in range(1,15):

        url = "https://www.flipkart.com/search?q={0}&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=on&as=off&page={1}".format(category, pageno)
        #url = "file:///data/Projects/flipkart/flipkart-gird/scrapers/flipkart/test.html"
        print(url)
        driver.get(url)
        rows = driver.find_elements_by_class_name('_3O0U0u')
        for row in rows:
        #for i in range(2):
            columns = row.find_elements_by_class_name('_3dqZjq')
            #columns = rows[i].find_elements_by_class_name('_3dqZjq')
            for column in columns:
                    results[category].append(column.get_attribute('href'))

with open("links.json", 'w') as file:
    json.dump(results, file)



# Commented out IPython magic to ensure Python compatibility.
# %cd ../flipkart/

def download_image(pic_urls, category, pid):
    try:
        if not os.path.exists("./data"):
            os.mkdir("./data")
        if not os.path.exists("./data/{0}".format(category)):
            os.mkdir("./data/{0}".format(category))
        for i, pic_url in enumerate(pic_urls):
            img_data = requests.get(pic_url).content
            with open('./data/{0}/{1}_{2}.jpg'.format(category, pid, i), 'wb') as handler:
                handler.write(img_data)
    except:
        print("This image failed to download")

#url = 'https://www.flipkart.com/metronaut-striped-men-polo-neck-red-t-shirt/p/itmfcg4yz4h8fmfe?pid=TSHFCG4Y6QEXCDRE&lid=LSTTSHFCG4Y6QEXCDREFYJBFG&marketplace=FLIPKART&srno=s_1_12&otracker=search&otracker1=search&fm=organic&iid=a8c90edf-845a-4557-ba6b-a6f2cbd3f3b3.TSHFCG4Y6QEXCDRE.SEARCH&ppt=sp&ppn=sp&ssid=c4wuta6df9alz7y81595325988773&qH=1d2b6a760c2e6c83'
#category="thirts_red"

with open("links.json", 'r') as file:
    inp = json.load(file);

extraction = {}
for category in inp:
    extraction[category] = []
    products = inp[category]
    for product in products:
    #for i in range(5):
        url = product
        print(url)
        parsed = urlparse.urlparse(url)

        pid = parse_qs(parsed.query)['pid'][0];
        ratings = ''
        stars = ''
        images = []

        driver.get(url);

#get the element containing stars
        stars = driver.find_elements_by_class_name('bqXGTW')

#get the element containing rating
        rating = driver.find_elements_by_class_name('_38sUEc')

#get the element containing price 
        price = driver.find_elements_by_class_name('_3qQ9m1')

#get the image url
        imageUrls = []
        try:
          imageUrls.append(driver.find_elements_by_class_name('_3wp706')[0].get_attribute('src'))
          download_image(imageUrls, category, pid)
        except:
          print('not possible for this image')

        obj = {}
        try:
            obj['pid'] = pid
        except:
            obj['pid'] = None

        try:
            obj['stars'] = stars[0].text
        except:
            obj['stars'] = None

        try:
            obj['rating'] = rating[0].text
        except:
            obj['rating'] = None 

        try:
            obj['price'] = price[0].text
        except:
            obj['price'] = None 

        extraction[category].append(obj)
        print(pid)
        print(obj['stars'])
        print(obj['rating'])
        print(obj['price'])
        print(imageUrls)


with open("dump.json", 'w') as file:
    json.dump(extraction, file)